{"title":"Beautifulsoup和requests爬取笔趣看","uid":"57287119be964990fe44e1874987acf7","slug":"Beautifulsoup和requests爬取笔趣看","date":"2021-07-01T03:46:00.000Z","updated":"2023-09-06T07:11:57.406Z","comments":true,"path":"api/articles/Beautifulsoup和requests爬取笔趣看.json","keywords":null,"cover":[],"content":"<span id=\"more\"></span>\n\n<p>这个是用来爬取笔趣看的一本书的所有文章，理论上可以成功地爬取笔趣看的每本书。</p>\n<p>关于讲解这个代码之前，我觉得需要注意的一些必要的内容：</p>\n<ul>\n<li>这个爬虫实质上就是利用某种工具帮助我们从网上爬取相应的HTML或其他结构，而一般来说，这个HTML或其他结构里面会有我们需要的内容，并且这些一般是<strong>字符串</strong>类型，也有可能是<strong>json</strong>类型。</li>\n<li>爬取到这些字符串后，我们接下来用<strong>Xpath</strong>、<strong>Beautifulsoup</strong>或<strong>PyQuery</strong>等等工具解析，因为这样会让我们更高效的找到自己所需要的内容。当然，我们可以不用这些，直接利用字符串的操作函数来找，比如python的split()等等，不过我觉得会比较费事和费力的，我个人不太推荐这个方式的。如果没有特殊的情况的话，还是先用这些解析工具比较好。</li>\n<li>最后利用这些解析工具或用自己的方式找到这些需要的数据以某种形式保存下去，保存的方式有：保存到csv文件，json文件，txt文件，以及保存到MySQL、MongoDB、Redis等等，选自己喜欢的就好。</li>\n<li>这个爬虫，我们需要面对静态页面和有js加载的动态页面，而前者会更简单，可以“可见即可爬”，后者比较困难，不能做到“可见即可爬”，需要利用Google等浏览器的开发者工具来帮助我们查找，就算如此，也有可能碰壁，这时我们可以利用Selenium等工具来帮助我们查找，做到“可见即可爬”，所以，我们有必要区分这两个页面类型，从而选取合适的爬虫方式。</li>\n<li>当我们学爬虫到一定的程度时，我们最好先学习框架，比如Python的<strong>Scrapy</strong>或Java的<strong>WebMagic</strong>等等，因为这些优秀的框架可以帮助我们省下不少的爬虫功夫，直接用好的轮子总会比自己闷头造轮子更好的吧。</li>\n<li>最后爬虫的流程一般是：</li>\n</ul>\n<p>接下来讲解代码：</p>\n<ol>\n<li><p>首先我们可以打开Google，进入笔趣看的极品家丁的目录页，然后打开开发者工具（用Ctrl+Shift+I），确定需要爬取的各个标题的位置。</p>\n</li>\n<li><p>然后用requests.get(url) 这个url是目录页的链接，这个功能是爬取网页的源代码（在静态网页上是通用的，而动态网页也许是失效的），其中，为了反爬，特意添加headers，构建一个请求头，具体过程如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/liuweixu/cdn/master/Python-crawler/Beautifulsoup(1).png\"></p>\n<p>最后注意编码格式的问题，因为一般来说，中文网的编码格式是gbk的，而我们如果不注意这个情况，爬下来的源代码中涉及到的中文可能出现乱码，因此我们需要进行处理：text.encoding &#x3D; text.apparent_encoding,</p>\n<p>也可以用text.encoding &#x3D; “utf-8”。</p>\n</li>\n<li><p>然后就是利用Beautifulsoup解析了，关于Beautifulsoup的推荐链接是：<a href=\"https://cuiqingcai.com/5548.html\">崔庆才的博客</a>， 最后并把爬下来的各个标题的链接进行整理，用于接下来的爬虫。</p>\n</li>\n<li><p>用requests遍历，爬取每个链接里面的内容，其中注意“&amp;nbsp”，这个是\\xa0，最好替换掉，否则会爬不出内容，这个也是反爬的一个比较简单的措施。最后把爬取的内容一个一个的保存到txt文件就行。</p>\n</li>\n</ol>\n<p>代码见下面。</p>\n<div class=\"language-txt\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">txt</span><pre class=\"shiki material-theme-palenight\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #babed8\">import requests</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">from bs4 import BeautifulSoup</span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\">headers = &#123;</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\\</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">     Chrome/77.0.3865.90 Safari/537.36&quot;</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">&#125;</span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\">list_url = []</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">list_title = []</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">target = &quot;https://www.biqukan.com&quot; # 这个是用来和在目录页爬取的链接相结合，从而得到一个可以访问的链接。</span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\"># 爬取笔趣看的《极品家丁》的目录页</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">def getHtmlList(url):</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    text = requests.get(url, headers=headers)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    text.encoding = text.apparent_encoding</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    if text.status_code == 200:</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        text = text.text</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        soup = BeautifulSoup(text, &#39;lxml&#39;)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        soup = soup.prettify()</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        soup = BeautifulSoup(soup, &#39;lxml&#39;)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        div = soup.find_all(&quot;div&quot;, class_=&quot;listmain&quot;)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        a_soup = div[0].find_all(&quot;a&quot;)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        for item in a_soup:</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">            list_url.append(target + item.get(&quot;href&quot;))</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">            list_title.append(item.string.replace(&quot;\\n&quot;, &quot;&quot;).strip())</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    else:</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        return None</span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\"># 爬取每个目录文章的内容，由于在笔趣看的书中，每个目录的的内容格式是几乎一样的。</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">def getHtmlContent(url):</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    txt = requests.get(url, headers=headers)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    txt.encoding = txt.apparent_encoding</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    if txt.status_code == 200:</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        return txt.text</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    else:</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        return None</span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\"># 用Beautifulsoup解析爬取的内容，注意&quot;\\xa0&quot;。</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">def parse_content(txt):</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    soup = BeautifulSoup(txt, &#39;lxml&#39;)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    div = soup.find_all(&#39;div&#39;, class_=&#39;showtxt&#39;)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    text = div[0].text.replace(&quot; &quot;, &quot;\\n&quot;).replace(&quot;\\xa0&quot; * 8, &#39; &#39;)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    return text</span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\"># 保存到txt文件中</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">def save_text(text, path):</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    with open(&quot;./极品家丁/&#123;&#125;.txt&quot;.format(path), &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        f.writelines(text)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\">if __name__ == &#39;__main__&#39;:</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    url = &#39;https://www.biqukan.com/3_3053/&#39;</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    getHtmlList(url)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    list_url = list_url[13:]</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    list_title = list_title[13:]</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">    for i in range(len(list_url)):</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        text = getHtmlContent(list_url[i])</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        text = parse_content(text)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        save_text(text, list_title[i])</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        print(&quot;\\r&quot; + &quot;下载进度：&#123;:.2f&#125;%&quot;.format(i / len(list_url) * 100), end=&quot;&quot;, flush=True) # 这个可以实现原地刷新，flush=True 和\\r 这两个不能少。</span></span></code></pre></div>","text":" 这个是用来爬取笔趣看的一本书的所有文章，理论上可以成功地爬取笔趣看的每本书。 关于讲解这个代码之前，我觉得需要注意的一些必要的内容： 这个爬虫实质上就是利用某...","permalink":"/post/Beautifulsoup和requests爬取笔趣看","photos":[],"count_time":{"symbolsCount":"3.5k","symbolsTime":"3 mins."},"categories":[],"tags":[{"name":"爬虫","slug":"爬虫","count":6,"path":"api/tags/爬虫.json"}],"toc":"","author":{"name":"Aurora","slug":"blog-author","avatar":"","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"Beautifulsoup 简单爬取笔趣阁的一本小说","uid":"3c1bb9fa0166d0dcb073cf074128c466","slug":"Beautifulsoup-简单爬取笔趣阁的一本小说","date":"2021-07-03T03:23:00.000Z","updated":"2023-09-06T07:12:09.436Z","comments":true,"path":"api/articles/Beautifulsoup-简单爬取笔趣阁的一本小说.json","keywords":null,"cover":null,"text":"我写这个代码的主要目的是针对一个关于上面的题目的博文进行修改，其实，这个博文讲的不错，但是由于网站更新了，他的代码有些地方是不灵的。所以我就修改了一些。 这个博...","permalink":"/post/Beautifulsoup-简单爬取笔趣阁的一本小说","photos":[],"count_time":{"symbolsCount":"2.5k","symbolsTime":"2 mins."},"categories":[],"tags":[{"name":"爬虫","slug":"爬虫","count":6,"path":"api/tags/爬虫.json"}],"author":{"name":"Aurora","slug":"blog-author","avatar":"","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"(brute force, implementation)A. Fafa and his Company","uid":"34d456b5a83eea087e8a8a1bd513e081","slug":"brute-force-implementation-A-Fafa-and-his-Company","date":"2020-09-06T02:35:56.000Z","updated":"2023-09-06T07:13:11.750Z","comments":true,"path":"api/articles/brute-force-implementation-A-Fafa-and-his-Company.json","keywords":null,"cover":null,"text":"水题 题目链接：935A - Fafa and his Company A. Fafa and his CompanyFafa owns a company t...","permalink":"/post/brute-force-implementation-A-Fafa-and-his-Company","photos":[],"count_time":{"symbolsCount":"2.2k","symbolsTime":"2 mins."},"categories":[],"tags":[{"name":"刷题：","slug":"刷题：","count":1,"path":"api/tags/刷题：.json"}],"author":{"name":"Aurora","slug":"blog-author","avatar":"","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}